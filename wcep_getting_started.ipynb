{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wcep-getting-started.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwApybTwmNZ-"
      },
      "source": [
        "# Getting started with the WCEP dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHPqe76-mNkZ"
      },
      "source": [
        "## Clone repository & install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dXMMBBLWpBS"
      },
      "source": [
        "!git clone https://github.com/complementizer/wcep-mds-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3CmFmFdQ-8K"
      },
      "source": [
        "cd wcep-mds-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfhlv38Il0OH"
      },
      "source": [
        "!git checkout experiments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQy8mqVPPEY0"
      },
      "source": [
        "cd experiments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jiw_6QQFmDfi"
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6LR_Gj9mhdd"
      },
      "source": [
        "cd experiments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox9g3nTdmvo3"
      },
      "source": [
        "## Download dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmlyseD3m4JK"
      },
      "source": [
        "!mkdir WCEP\n",
        "!gdown https://drive.google.com/uc?id=1kUjSRXzKnTYdJ732BkKVLg3CFxDKo25u -O WCEP/train.jsonl.gz\n",
        "!gdown https://drive.google.com/uc?id=1_kHTZ32jazTbXaFRg0vBeIsVcpI7CTmy -O WCEP/val.jsonl.gz\n",
        "!gdown https://drive.google.com/uc?id=1qsd5pOCpeSXsaqNobXCrcAzhcjtG1wA1 -O WCEP/test.jsonl.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x1SC8oysCbd"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "We use the WCEP validation data as an example. <br> Each item in the dataset corresponds to a cluster of news articles about a news event and contains some metadata, most importantly the ground-truth summary for the cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_pgsRlisGVY"
      },
      "source": [
        "import utils\n",
        "\n",
        "val_data = list(utils.read_jsonl_gz('WCEP/val.jsonl.gz'))\n",
        "\n",
        "print(val_data[0].keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBtm1KELsmP0"
      },
      "source": [
        "## Run extractive baselines & oracles\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6ygQepSs9mu"
      },
      "source": [
        "from baselines import RandomBaseline, TextRankSummarizer, CentroidSummarizer, SubmodularSummarizer\n",
        "from oracles import Oracle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7DOOKFFvbvD"
      },
      "source": [
        "First we create summarizer objects and set their hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv-rJXfns_VJ"
      },
      "source": [
        "random_sum = RandomBaseline()\n",
        "textrank = TextRankSummarizer(max_redundancy=0.5)\n",
        "centroid = CentroidSummarizer(max_redundancy=0.5)\n",
        "submod = SubmodularSummarizer(a=5, div_weight=6, cluster_factor=0.2) # div_weight encourages diversity/non-reduncancy\n",
        "oracle = Oracle()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqQFXerDvziU"
      },
      "source": [
        "Below we pick one set of settings for extractive summarization that we will use for all baselines. <br>\n",
        "* `in_titles` means we add article titles as sentences in the input, and `out_titles` means we also allow these titles to be part of a summary\n",
        "* we set a minimum sentence length (`min_sent_tokens`) because short broken sentences appear frequently and are usually not desirable\n",
        "* you can set the length contraint to `words`, `sents` or `chars`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlnPI2ZXuQVq"
      },
      "source": [
        "settings = {\n",
        "    'max_len': 40, 'len_type': 'words',\n",
        "    'in_titles': False, 'out_titles': False,\n",
        "    'min_sent_tokens': 7, 'max_sent_tokens': 40,    \n",
        "}\n",
        "max_articles = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eab554ZmwfuQ"
      },
      "source": [
        "For a quick experiment, we only select the first 10 clusters of the WCEP validation data and use the first 10 articles of each cluster as inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQjaFzGxuSfQ"
      },
      "source": [
        "example_clusters = [c['articles'][:max_articles] for c in val_data[:10]]\n",
        "ref_summaries = [c['summary'] for c in val_data[:10]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idc7XbPluS8u"
      },
      "source": [
        "textrank_summaries = [textrank.summarize(articles, **settings) for articles in example_clusters]\n",
        "centroid_summaries = [centroid.summarize(articles, **settings) for articles in example_clusters]\n",
        "submod_summaries = [submod.summarize(articles, **settings) for articles in example_clusters]\n",
        "random_summaries = [random_sum.summarize(articles, **settings) for articles in example_clusters]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsI0U_GFxVO2"
      },
      "source": [
        "oracle_summaries = [oracle.summarize(ref, articles, **settings)\n",
        "                    for (ref, articles) in zip(ref_summaries, example_clusters)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0uLcSCJuwfJ"
      },
      "source": [
        "## Evaluate summaries\n",
        "\n",
        "**Note:** our `evaluate` function uses a wrapper from the [newsroom library](https://github.com/lil-lab/newsroom) to compute ROUGE scores.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdEhYT7vuy2a"
      },
      "source": [
        "from pprint import pprint\n",
        "from evaluate import evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMtIIDhhu0ui"
      },
      "source": [
        "names = ['TextRank', 'Centroid', 'Submodular', 'Oracle', 'Random']\n",
        "outputs = [textrank_summaries, centroid_summaries, submod_summaries, oracle_summaries, random_summaries]\n",
        "\n",
        "for preds, name in zip(outputs, names):\n",
        "    print(name)\n",
        "    results = evaluate(ref_summaries, preds, lowercase=True)\n",
        "    pprint(results)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_6WUfV-wwBd"
      },
      "source": [
        "Let's look at some example summaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0hgVBJ8wvGT"
      },
      "source": [
        "cluster_idx = 6\n",
        "print('Ground-truth')\n",
        "print(ref_summaries[cluster_idx])\n",
        "print()\n",
        "\n",
        "for preds, name in zip(outputs, names):\n",
        "    print(name)\n",
        "    print(preds[cluster_idx])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7qLELVUXlZX"
      },
      "source": [
        "### Blog Example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwqdZ5R-YQ9s"
      },
      "source": [
        "from utils import read_jsonl_gz\n",
        "from baselines import TextRankSummarizer\n",
        "from evaluate import evaluate\n",
        "from pprint import pprint\n",
        "\n",
        "textrank = TextRankSummarizer()\n",
        "\n",
        "dataset = list(read_jsonl_gz('WCEP/val.jsonl.gz'))\n",
        "cluster = dataset[954]\n",
        "articles = cluster['articles'][:10]\n",
        "\n",
        "human_summary = cluster['summary']\n",
        "automatic_summary = textrank.summarize(articles)\n",
        "results = evaluate([human_summary], [automatic_summary])\n",
        "\n",
        "print('Summary:')\n",
        "print(automatic_summary)\n",
        "print()\n",
        "print('ROUGE scores:')\n",
        "pprint(results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}